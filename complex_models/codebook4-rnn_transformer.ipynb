{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba0c467-10a9-474d-a675-51210c5f4012",
   "metadata": {},
   "source": [
    "## Transformer for Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938a1f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6289d2b9-b8f9-433c-b9f5-26709e638234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02994fb0-08b7-4b96-ac46-533f97f77843",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('award_data.csv')\n",
    "data.iloc[0]['Abstract'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe6005-b967-4a25-95c1-92005fbcbc84",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Replace the dataset with the actual DataFrame you have\n",
    "# Standardize the State column to uppercase and strip extra spaces\n",
    "data['State'] = data['State'].str.strip().str.upper()\n",
    "\n",
    "# Check unique states after cleaning\n",
    "unique_states = data['State'].unique()\n",
    "print(\"Unique States After Cleaning:\", unique_states)\n",
    "\n",
    "# If you need to handle specific cases (e.g., replace 'NAN' with 'Unknown' or drop them):\n",
    "data['State'].replace('NAN', pd.NA, inplace=True)  # Convert 'nan' to proper missing values\n",
    "data.dropna(subset=['State'], inplace=True)  # Drop rows with missing State if necessary\n",
    "\n",
    "# Re-check unique states to verify cleaning\n",
    "unique_states_cleaned = data['State'].unique()\n",
    "print(\"Final Unique States:\", unique_states_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8561fa92-3a71-4983-b639-377fb9dcee69",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the data has been loaded into a DataFrame called 'data'\n",
    "# Replace 'State Column Name' and 'Proposal Title Column Name' with actual column names in your dataset\n",
    "state_column = 'State'  # Replace with the actual column name for state\n",
    "proposal_title_column = 'Abstract'  # Replace with the actual column name for proposal titles\n",
    "\n",
    "# Normalizing the state names to uppercase to handle duplicates due to case sensitivity\n",
    "data[state_column] = data[state_column].str.upper()\n",
    "\n",
    "# Creating a dictionary with states as keys and lists of proposal titles as values\n",
    "state_proposals = data.groupby(state_column)[proposal_title_column].apply(list).to_dict()\n",
    "\n",
    "# Display the resulting dictionary\n",
    "print(state_proposals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b4703-08c2-4c1c-ba4f-8d38c1e1bc1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "len(state_proposals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca8453-f077-40ed-88d3-49a32919eec3",
   "metadata": {},
   "source": [
    "# Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc806e-67d8-4733-bb4c-acc0e7ad12c5",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "Raw text needs to be converted into numeric tensors before feeding to a neural network. We will perform the following preprocessing steps:\n",
    "- Loading and Tokenization: Read the review text files and split the text into tokens (words). Clean the text by lowercasing and removing punctuation.\n",
    "- Vocabulary Creation: Build a vocabulary of the most frequent words, mapping each word to an integer index.\n",
    "- Encoding: Convert each review into a sequence of word indices using the vocabulary (unknown words get a special index).\n",
    "- Padding/Truncation: Pad or truncate each sequence to a fixed length so they can be batched into tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30044885-a138-446f-9815-ad67bc15cea7",
   "metadata": {},
   "source": [
    "## Loading and Tokenizing Reviews\n",
    "We'll gather all training reviews and their labels first. The training data will be used to build the vocabulary. The IMDb dataset comes with train/test split, so we'll keep those separate. To load the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe2b47-1903-4689-9236-c4883e4df278",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "len(state_proposals['TN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c789fcf-2a8d-4583-84fb-3249b4a47882",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "OR_Proposal = state_proposals['OR']\n",
    "TN_Proposal = state_proposals['TN']\n",
    "\n",
    "Train_OR_Proposal = OR_Proposal[:len(OR_Proposal)//4*3]\n",
    "Test_OR_Proposal = OR_Proposal[len(OR_Proposal)//4*3:]\n",
    "\n",
    "Train_TN_Proposal = TN_Proposal[:len(TN_Proposal)//4*3]\n",
    "Test_TN_Proposal = TN_Proposal[len(TN_Proposal)//4*3:]\n",
    "\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "\n",
    "# Load and label positive training reviews\n",
    "for text in Train_OR_Proposal:\n",
    "    train_texts.append(str(text))\n",
    "    train_labels.append(1)  # positive label 1\n",
    "\n",
    "# Load and label negative training reviews\n",
    "for text in Train_TN_Proposal:\n",
    "    train_texts.append(str(text))\n",
    "    train_labels.append(0)  # negative label 0\n",
    "\n",
    "# Load and label positive test reviews\n",
    "for text in Test_OR_Proposal:\n",
    "    test_texts.append(str(text))\n",
    "    test_labels.append(1)  # positive label 1\n",
    "\n",
    "# Load and label negative test reviews\n",
    "for text in Test_TN_Proposal:\n",
    "    test_texts.append(str(text))\n",
    "    test_labels.append(0)  # negative label 0\n",
    "\n",
    "print(f\"Loaded {len(train_texts)} training reviews and {len(test_texts)} test reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ac98b-7a1c-46e6-a26e-8772d18071ab",
   "metadata": {},
   "source": [
    "Next, we define a function to tokenize and clean a review string. We'll remove HTML tags (if any), punctuation, and make everything lowercase. We'll use a simple regex to keep only letters and numbers (you could also use more advanced tokenizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314cc2e-cc1f-4e99-8c27-9e30d0482315",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    # Keep only letters and standard punctuation (replace others with space)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", ' ', text)\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Split into tokens by whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Tokenize all reviews\n",
    "train_tokens = [tokenize(review) for review in train_texts]\n",
    "test_tokens  = [tokenize(review) for review in test_texts]\n",
    "\n",
    "# Peek at one tokenized example\n",
    "print(train_texts[0][:100], \"->\", train_tokens[0][:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e9b73-599d-42d8-90fd-1804cda8311d",
   "metadata": {},
   "source": [
    "## Building the Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0b605-3ba4-4629-b7ec-f0a38d8e0114",
   "metadata": {},
   "source": [
    "Now we build a vocabulary (word index mapping) based on the training tokens. It's common to limit the vocabulary size to the top N most frequent words to avoid very rare words. In this dataset, there are around 130k unique tokens if we include everything. Using all unique words can hurt performance (very sparse, many rare words), so we will limit to the most frequent, for example, 10,000 words. (This is a common choice in literature, often 10k or 20k most common words.) Words outside this top list will be treated as \"unknown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbd78b-0cba-4af7-982e-44d70ba9438b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count frequency of each token in the training set\n",
    "word_counts = Counter(token for review in train_tokens for token in review)\n",
    "print(f\"Total unique tokens in training data: {len(word_counts)}\")\n",
    "\n",
    "# Select most common words for the vocabulary\n",
    "vocab_size = 10000  # limit vocab to top 10k\n",
    "most_common = word_counts.most_common(vocab_size - 2)  # -2 to account for special tokens\n",
    "# We will reserve indices 0 and 1 for special tokens (<PAD> and <UNK>)\n",
    "word_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for i, (word, freq) in enumerate(most_common, start=2):\n",
    "    word_to_idx[word] = i\n",
    "\n",
    "print(f\"Vocabulary size (including PAD/UNK): {len(word_to_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a3023-c244-47b1-b902-e072f19065d3",
   "metadata": {},
   "source": [
    "We added two special tokens: <PAD> for padding and <UNK> for any unknown/out-of-vocabulary word. We assigned <PAD> index 0 (we will use 0 for padding in sequences) and <UNK> index 1. Common words like \"the\", \"and\", etc., will get low indices since they appear frequent. For example, \"the\" might be index 2, \"and\" 3, etc., depending on frequencies. This frequency-based indexing makes it easy to filter out rare words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d204ba8-a76b-417a-a4f3-840daa653cda",
   "metadata": {},
   "source": [
    "## Encoding and Padding Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5763b5-c5eb-42ae-84f7-60cb6ae82c0b",
   "metadata": {},
   "source": [
    "With the vocabulary in hand, we encode each review’s token list into a sequence of integers. Words not in our word_to_idx (e.g., a rare word not in top 10k) will be mapped to <UNK> (index 1).We also need to pad or truncate each sequence to a fixed length. Neural networks typically process batches of equal-length sequences. We'll pick a maximum sequence length (for example, 250 words). The average IMDb review is around 230 words, and most reviews are under 500 words, so 250 is a reasonable cutoff for this tutorial. Reviews longer than 250 words will be truncated, and shorter reviews will be padded with <PAD> (zeros) at the end. (Padding at the end is common for LSTMs/Transformers; padding at the beginning is another option but either works if handled consistently.)Let's encode and pad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b02ce4-6159-42fe-9b41-f7d2921cffc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "max_length = 250\n",
    "\n",
    "def encode_and_pad(tokens):\n",
    "    # Encode tokens to indices\n",
    "    indices = [word_to_idx.get(token, 1) for token in tokens]  # 1 is <UNK> for unknown\n",
    "    # Truncate if longer than max_length\n",
    "    if len(indices) > max_length:\n",
    "        indices = indices[:max_length]\n",
    "    # Pad with 0s if shorter than max_length\n",
    "    if len(indices) < max_length:\n",
    "        indices += [0] * (max_length - len(indices))\n",
    "    return indices\n",
    "\n",
    "# Encode all training and test sequences\n",
    "train_sequences = [encode_and_pad(tok_list) for tok_list in train_tokens]\n",
    "test_sequences  = [encode_and_pad(tok_list) for tok_list in test_tokens]\n",
    "\n",
    "print(\"Example encoded review (first 20 indices):\", train_sequences[0][:20])\n",
    "print(\"Length of encoded review:\", len(train_sequences[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cc9c04-1eb4-44b5-b189-34b88b6103e4",
   "metadata": {},
   "source": [
    "Now we have train_sequences and test_sequences which are lists of equal-length lists of integers (each length = 250). Each integer is an index in our vocabulary. For example, if \"the\" is index 2, you'll see \"2\" wherever \"the\" appeared in the text. Unknown words will appear as \"1\". And padded positions are \"0\".At this point, our data is ready to be fed into PyTorch models. We just need to wrap it in a PyTorch Dataset and DataLoader for convenient batching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84aaf6-0346-4f8b-9df2-ac52bd4b607e",
   "metadata": {},
   "source": [
    "## Creating PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e3b72-c39b-43b3-89e8-557f970b5f29",
   "metadata": {},
   "source": [
    "We'll create a custom Dataset to hold our encoded sequences and labels, and then use DataLoader to handle batching and shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbad7af-399b-4fa9-a539-8164afd00778",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ProposalDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "        self.labels    = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = ProposalDataset(train_sequences, train_labels)\n",
    "test_dataset  = ProposalDataset(test_sequences, test_labels)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f85c6-755d-4e00-9e3b-0bc00ff81e18",
   "metadata": {},
   "source": [
    "We use shuffle=True for training data so that each epoch the model sees batches in a new order (which helps training). We keep shuffle=False for the test loader to evaluate in a fixed order (shuffling isn’t necessary for evaluation).Now we're ready to define our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe674b-cc4e-4eb9-b2c1-cbb1a881577f",
   "metadata": {},
   "source": [
    "## Defining the Models\n",
    "We'll implement two models using PyTorch's nn.Module:\n",
    "\n",
    "1. LSTM-based RNN: An Embedding layer followed by an LSTM (recurrent layer) and a linear layer to output the sentiment class.\n",
    "2. Transformer Encoder: An Embedding layer (with added positional encoding) followed by Transformer encoder layers and a final linear layer for classification.\n",
    "\n",
    "Both models will output a prediction for each input review (binary classification: positive or negative). We will use a size-2 output (for classes 0 and 1) and later apply a softmax or use CrossEntropyLoss which expects raw logits of size 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0baa68-537a-4132-9eb1-13b80d45b6e7",
   "metadata": {},
   "source": [
    "## Transformer Encoder Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e8816-2c39-4bcc-9d36-d27eec1fb3b3",
   "metadata": {},
   "source": [
    "Transformers use self-attention to process all words in a sequence in parallel (not strictly left-to-right like an RNN). We will implement a Transformer encoder that produces contextualized embeddings for each position, then aggregate those into a single vector for classification. One common approach is to prepend a special [CLS] token and use its output embedding for classification (like BERT does), but for simplicity, we'll instead average the transformer outputs from all positions.One challenge is that Transformers are position-agnostic, so we need to provide positional information. We'll add a positional embedding to the token embeddings.Let's define a TransformerClassifier module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee82194b-7748-47b5-bd06-7b547e8b357e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, num_heads=4, num_layers=2, ff_dim=256, max_len=250):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_embedding = nn.Embedding(max_len, embed_dim)  # learnable positional embeddings\n",
    "        # Define a Transformer Encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, \n",
    "                                                  dim_feedforward=ff_dim, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, 2)\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_length)\n",
    "        batch_size, seq_len = x.size()\n",
    "        # Create position indices for each position in the sequence\n",
    "        pos_indices = torch.arange(0, seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        # Get token embeddings and positional embeddings\n",
    "        token_embeds = self.embedding(x)            # (batch, seq_len, embed_dim)\n",
    "        pos_embeds = self.pos_embedding(pos_indices)  # (batch, seq_len, embed_dim)\n",
    "        # Add token and position embeddings\n",
    "        x_embedded = token_embeds + pos_embeds       # (batch, seq_len, embed_dim)\n",
    "        # Create padding mask (True where padding token is present)\n",
    "        pad_mask = (x == 0)  # shape: (batch, seq_len), True at padded indices\n",
    "        # Pass through Transformer encoder\n",
    "        enc_out = self.transformer(x_embedded, src_key_padding_mask=pad_mask)  # (batch, seq_len, embed_dim)\n",
    "        # Aggregate the encoder outputs; we use mean pooling\n",
    "        seq_avg = enc_out.mean(dim=1)               # (batch, embed_dim)\n",
    "        logits = self.fc(seq_avg)                   # (batch, 2)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967f18f-cb96-4f11-a0d4-c0409f05402f",
   "metadata": {},
   "source": [
    "Key points for the Transformer model:\n",
    "\n",
    "- We use nn.TransformerEncoderLayer and stack two layers (you can adjust num_layers). We set batch_first=True so it expects input shape (batch, seq, embed).\n",
    "- We use a small number of heads (4) and feed-forward dimension (256) for demonstration. In practice, you might use more heads or larger dimensions.\n",
    "- We add positional embeddings to the word embeddings to give the model a sense of word order. These are learned embeddings for positions 0 to max_len-1.\n",
    "- We create a pad_mask where positions with token index 0 (PAD) are marked True, so the Transformer will ignore those positions in its self-attention calculations. This ensures padded tokens don't affect the attention or outputs.\n",
    "- After the Transformer encoder, we average (mean) the output vectors across the sequence length to get a single vector per sequence. (Alternatively, one could use the output at the first position, use max-pooling, or a designated [CLS] token output. Averaging is a simple way to get a global representation.)\n",
    "- Then we apply a linear layer to get class logits.\n",
    "\n",
    "Now we have both models defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e47582-f2d4-4641-827c-d5a97578d063",
   "metadata": {},
   "source": [
    "## Training the Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9ec323-fb57-4449-813d-5e9cf6bcd009",
   "metadata": {},
   "source": [
    "We will train and evaluate the LSTM model first, then the Transformer model. Both will be trained from scratch on our dataset. We will use a simple training loop without any high-level libraries.First, set up the training essentials: loss function, optimizer, and device (CPU/GPU). We'll use CrossEntropyLoss since it's a classification task with logits, and Adam optimizer for both models. If a GPU is available, we'll use it for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f2bd52-ac8d-4370-8a43-1a5d5238038e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Select device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Instantiate models\n",
    "vocab_size = len(word_to_idx)\n",
    "transformer_model = TransformerClassifier(vocab_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trans_optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8e7f83-3ccb-4c06-b3dd-1979c042a925",
   "metadata": {},
   "source": [
    "## Training the Transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b5bfa-ff84-4824-b32e-bb896642c018",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    transformer_model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        trans_optimizer.zero_grad()\n",
    "        outputs = transformer_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        trans_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Transformer Training loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d6535-3af5-4dc3-8fef-f3ebb25d3776",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "transformer_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = transformer_model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "accuracy = correct / total\n",
    "print(f\"Transformer Model Test Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
