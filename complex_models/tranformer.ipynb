{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataframe \"data\" and grab the first lines abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_316447/1612965084.py:2: DtypeWarning: Columns (10,38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('award_data.csv') # loads and parses award data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The Department of Homeland Security (DHS) grapples with vast and diverse datasets collected daily, ranging from personal property scans to Stream of Commerce (SoC) data. To analyze and improve algorithms for detecting explosives and prohibited items, efficient curation and labeling are essential. However, DHS faces challenges, including data processing inefficiencies, dependency on human labeling, limited scalability, predictive analytics and threat detection obstacles, and inter-agency collaboration barriers.In response, Agile Data Decisions, Inc. (AgileDD) proposes an innovative solution called AI for Data Labeling and Curation at Scale (AI-DLCS). Leveraging their iQC human-in-the-loop AI platform and the CargoSeer AI platform, the project aims to address DHS's challenges. CargoSeer AI, developed by CargoSeer LTD, specializes in consignment inspection, utilizing a Large Foundation Model to automatically inspect scanned cargo for fraud. AgileDD plans to enhance these platforms with new algorithms for (1) labeling at scale from a known single image with few-shot learning, and (2) multi-class/multi-label image classification and object detection with weakly supervised learning.The technical objectives of the proposed Phase I research include developing a data ingestion and pre-processing pipeline for diverse image and document formats, establishing standardized metrics for auto-labeling, implementing large-scale auto-labeling with few-shot learning, conducting multi-label and multi-class auto-labeling on a large dataset, and demonstrating a proof-of-concept workflow on the ImageNet dataset. The goal is to enhance efficiency, reduce human dependency, and improve scalability for DHS in handling complex and extensive datasets crucial for security and defense decision-making. The proposed solution showcases promise in revolutionizing data handling processes for security applications.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe\n",
    "data = pd.read_csv('award_data.csv') # loads and parses award data\n",
    "data.iloc[0]['Abstract'] # prints out first lines abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize data, get rid of data objects with NaN as the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Unique States: ['TX' 'CA' 'VT' 'MD' 'GA' 'MI' 'MA' 'NC' 'FL' 'CO' 'OH' 'CT' 'IL' 'AK'\n",
      " 'NM' 'VA' 'OR' 'AL' 'NY' 'PA' 'WY' 'WI' 'MN' 'NV' 'IN' 'AZ' 'ID' 'KY'\n",
      " 'DE' 'NJ' 'SC' 'MT' 'DC' 'MO' 'NE' 'HI' 'ND' 'WA' 'TN' 'UT' 'WV' 'OK'\n",
      " 'RI' 'AR' 'KS' 'LA' 'IA' 'ME' 'NH' 'MS' 'PR' 'SD' 'AS' 'MH' 'VI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_316447/3455209180.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['State'].replace('NAN', pd.NA, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# convert state column to uppercase and removes \\n and \" \"\n",
    "data['State'] = data['State'].str.strip().str.upper()\n",
    "# get unique states after cleaning\n",
    "unique_states = data['State'].unique()\n",
    "print(\"Unique States After Cleaning:\", unique_states)\n",
    "# replace NAN with pd.NA\n",
    "data['State'].replace('NAN', pd.NA, inplace=True) \n",
    "# removes all rows where state is pd.NA\n",
    "data.dropna(subset=['State'], inplace=True)\n",
    "# re-check unique states to verify cleaning\n",
    "unique_states_cleaned = data['State'].unique()\n",
    "print(\"Final Unique States:\", unique_states_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a dictionary d[state] = list of abstract associated with that state:\n",
    "Ex: \n",
    "{\n",
    "    \"CALIFORNIA\": [\"Abstract 1\", \"Abstract 2\", \"Abstract 3\"],\n",
    "    \"TEXAS\": [\"Abstract A\", \"Abstract B\"],\n",
    "    \"OREGON\": [\"Abstract X\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "state_column = 'State'\n",
    "proposal_title_column = 'Abstract'\n",
    "data[state_column] = data[state_column].str.upper()\n",
    "\"\"\"\n",
    "data.groupby(state_column)\n",
    "    creates grouped object\n",
    "    rows are grouped together by their states\n",
    "data.groupby(state_column)[proposal_title_column]\n",
    "    select only the title column\n",
    "data.groupby(state_column)[proposal_title_column].apply(list)\n",
    "    converts each state's groups of absrtacts into a list\n",
    "data.groupby(state_column)[proposal_title_column].apply(list).to_dict()\n",
    "    converts to a dictionary: dict[state] = list of abstract of that state\n",
    "\"\"\"\n",
    "state_proposals = data.groupby(state_column)[proposal_title_column].apply(list).to_dict()\n",
    "print(len(state_proposals))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
