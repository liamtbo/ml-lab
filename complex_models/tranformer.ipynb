{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataframe \"data\" and grab the first lines abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413032/1612965084.py:2: DtypeWarning: Columns (10,38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('award_data.csv') # loads and parses award data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The Department of Homeland Security (DHS) grapples with vast and diverse datasets collected daily, ranging from personal property scans to Stream of Commerce (SoC) data. To analyze and improve algorithms for detecting explosives and prohibited items, efficient curation and labeling are essential. However, DHS faces challenges, including data processing inefficiencies, dependency on human labeling, limited scalability, predictive analytics and threat detection obstacles, and inter-agency collaboration barriers.In response, Agile Data Decisions, Inc. (AgileDD) proposes an innovative solution called AI for Data Labeling and Curation at Scale (AI-DLCS). Leveraging their iQC human-in-the-loop AI platform and the CargoSeer AI platform, the project aims to address DHS's challenges. CargoSeer AI, developed by CargoSeer LTD, specializes in consignment inspection, utilizing a Large Foundation Model to automatically inspect scanned cargo for fraud. AgileDD plans to enhance these platforms with new algorithms for (1) labeling at scale from a known single image with few-shot learning, and (2) multi-class/multi-label image classification and object detection with weakly supervised learning.The technical objectives of the proposed Phase I research include developing a data ingestion and pre-processing pipeline for diverse image and document formats, establishing standardized metrics for auto-labeling, implementing large-scale auto-labeling with few-shot learning, conducting multi-label and multi-class auto-labeling on a large dataset, and demonstrating a proof-of-concept workflow on the ImageNet dataset. The goal is to enhance efficiency, reduce human dependency, and improve scalability for DHS in handling complex and extensive datasets crucial for security and defense decision-making. The proposed solution showcases promise in revolutionizing data handling processes for security applications.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe\n",
    "data = pd.read_csv('award_data.csv') # loads and parses award data\n",
    "data.iloc[0]['Abstract'] # prints out first lines abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize data, get rid of data objects with NaN as the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique States After Cleaning: ['TX' 'CA' 'VT' 'MD' 'GA' 'MI' 'MA' 'NC' 'FL' 'CO' 'OH' 'CT' 'IL' 'AK'\n",
      " 'NM' 'VA' 'OR' 'AL' 'NY' 'PA' 'WY' 'WI' 'MN' 'NV' 'IN' 'AZ' 'ID' 'KY'\n",
      " 'DE' 'NJ' 'SC' 'MT' 'DC' 'MO' 'NE' 'HI' 'ND' 'WA' 'TN' 'UT' 'WV' 'OK'\n",
      " 'RI' 'AR' 'KS' 'LA' 'IA' 'ME' 'NH' 'MS' 'PR' 'SD' 'AS' 'MH' nan 'VI']\n",
      "Final Unique States: ['TX' 'CA' 'VT' 'MD' 'GA' 'MI' 'MA' 'NC' 'FL' 'CO' 'OH' 'CT' 'IL' 'AK'\n",
      " 'NM' 'VA' 'OR' 'AL' 'NY' 'PA' 'WY' 'WI' 'MN' 'NV' 'IN' 'AZ' 'ID' 'KY'\n",
      " 'DE' 'NJ' 'SC' 'MT' 'DC' 'MO' 'NE' 'HI' 'ND' 'WA' 'TN' 'UT' 'WV' 'OK'\n",
      " 'RI' 'AR' 'KS' 'LA' 'IA' 'ME' 'NH' 'MS' 'PR' 'SD' 'AS' 'MH' 'VI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413032/37711856.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['State'].replace('NAN', pd.NA, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# convert state column to uppercase and removes \\n and \" \"\n",
    "data['State'] = data['State'].str.strip().str.upper()\n",
    "# get unique states after cleaning\n",
    "unique_states = data['State'].unique()\n",
    "print(\"Unique States After Cleaning:\", unique_states)\n",
    "# replace NAN with pd.NA\n",
    "data['State'].replace('NAN', pd.NA, inplace=True) \n",
    "# removes all rows where state is pd.NA\n",
    "data.dropna(subset=['State'], inplace=True)\n",
    "# re-check unique states to verify cleaning\n",
    "unique_states_cleaned = data['State'].unique()\n",
    "print(\"Final Unique States:\", unique_states_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a dictionary d[state] = list of abstract associated with that state:\n",
    "Ex: \n",
    "{\n",
    "    \"CALIFORNIA\": [\"Abstract 1\", \"Abstract 2\", \"Abstract 3\"],\n",
    "    \"TEXAS\": [\"Abstract A\", \"Abstract B\"],\n",
    "    \"OREGON\": [\"Abstract X\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "state_column = 'State'\n",
    "proposal_title_column = 'Abstract'\n",
    "data[state_column] = data[state_column].str.upper()\n",
    "\"\"\"\n",
    "data.groupby(state_column)\n",
    "    creates grouped object\n",
    "    rows are grouped together by their states\n",
    "data.groupby(state_column)[proposal_title_column]\n",
    "    select only the title column\n",
    "data.groupby(state_column)[proposal_title_column].apply(list)\n",
    "    converts each state's groups of absrtacts into a list\n",
    "data.groupby(state_column)[proposal_title_column].apply(list).to_dict()\n",
    "    converts to a dictionary: dict[state] = list of abstract of that state\n",
    "\"\"\"\n",
    "state_proposals = data.groupby(state_column)[proposal_title_column].apply(list).to_dict()\n",
    "print(len(state_proposals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Tokenizing Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1709"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_proposals['TN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split up the training (75%) and the test data (25%). Also create parallel ground truth arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3240 training reviews and 1082 test reviews.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "OR_Proposal = state_proposals['OR']\n",
    "TN_Proposal = state_proposals['TN']\n",
    "\n",
    "# split up the training and test data by 3/4 and 1/4 respectively\n",
    "Train_OR_Proposal = OR_Proposal[:len(OR_Proposal)//4*3]\n",
    "Test_OR_Proposal = OR_Proposal[len(OR_Proposal)//4*3:]\n",
    "Train_TN_Proposal = TN_Proposal[:len(TN_Proposal)//4*3]\n",
    "Test_TN_Proposal = TN_Proposal[len(TN_Proposal)//4*3:]\n",
    "\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "\n",
    "for test in Train_OR_Proposal:\n",
    "    train_texts.append(str(test))\n",
    "    train_labels.append(1)\n",
    "\n",
    "# Load and label negative training reviews\n",
    "for text in Train_TN_Proposal:\n",
    "    train_texts.append(str(text))\n",
    "    train_labels.append(0)  # negative label 0\n",
    "\n",
    "# Load and label positive test reviews\n",
    "for text in Test_OR_Proposal:\n",
    "    test_texts.append(str(text))\n",
    "    test_labels.append(1)  # positive label 1\n",
    "\n",
    "# Load and label negative test reviews\n",
    "for text in Test_TN_Proposal:\n",
    "    test_texts.append(str(text))\n",
    "    test_labels.append(0)  # negative label 0\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(train_texts)} training reviews and {len(test_texts)} test reviews.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the Artemis program, NASA currently plans to land the first woman and next man on the moon by 2 -> ['with', 'the', 'artemis', 'program', 'nasa', 'currently', 'plans', 'to', 'land', 'the', 'first', 'woman', 'and', 'next', 'man', 'on', 'the', 'moon', 'by', '2025']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    # Keep only letters and standard punctuation (replace others with space)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", ' ', text)\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Split into tokens by whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Tokenize all reviews\n",
    "train_tokens = [tokenize(review) for review in train_texts]\n",
    "test_tokens  = [tokenize(review) for review in test_texts]\n",
    "\n",
    "# Peek at one tokenized example\n",
    "print(train_texts[0][:100], \"->\", train_tokens[0][:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Vocabulary\n",
    "Here we're grabbing the most common 10,000 words. This removes rarer words, reducing dimensionality and making learning easier. <PAD> will hold padding introduced (necessary for tensors since all need to be the same size) and <UNK> will hold unknown/out-of-vocab words.\n",
    "We're also assigning indexes to each word, with more frequent words takng lower indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens in training data: 28888\n",
      "Vocabulary size (includingn PAD/UNK): 10000\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# bag of words - counts frequency of each token in the training set\n",
    "word_counts = Counter(token for review in train_tokens for token in review)\n",
    "print(f\"Total unique tokens in training data: {len(word_counts)}\")\n",
    "\n",
    "vocab_size = 10_000 # limit vocab size to the to 10k\n",
    "# returns 10_000 most common words\n",
    "most_common = word_counts.most_common(vocab_size - 2) # -2 is for special tokens (<PAD> and <UNK>)\n",
    "# each unique word gets its own unique index\n",
    "word_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for i, (word, freq) in enumerate(most_common, start=2):\n",
    "    word_to_idx[word] = i\n",
    "\n",
    "print(f\"Vocabulary size (includingn PAD/UNK): {len(word_to_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Padding Sequences\n",
    "\n",
    "For each abstract (list of tokens), we're going to create an array of unique indexes that correspond to each unique word. We will also pad / truncate till each returned array is 250 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example encoded review (first 20 indices): [12, 2, 9977, 39, 634, 152, 1486, 5, 2765, 2, 166, 6998, 3, 582, 2766, 19, 2, 4433, 15, 8956]\n",
      "Length of encoded review: 250\n"
     ]
    }
   ],
   "source": [
    "max_length = 250\n",
    "def encode_and_pad(tokens):\n",
    "    # populate indices with tokens unique index, if token doesn't have an index, set it to 1 <UNK>\n",
    "    indices = [word_to_idx.get(token, 1) for token in tokens] # 1 is for <UNK> for unknown\n",
    "    if len(indices) > max_length:\n",
    "        indices = indices[:max_length]\n",
    "    # if length of abstracti < 250 then make the rest 0's which is identified as padding\n",
    "    if len(indices) < max_length:\n",
    "           indices += [0] * (max_length - len(indices))\n",
    "    return indices\n",
    "\n",
    "train_sequences = [encode_and_pad(tok_list) for tok_list in train_tokens]\n",
    "test_sequences = [encode_and_pad(tok_list) for tok_list in test_tokens]\n",
    "\n",
    "print(\"Example encoded review (first 20 indices):\", train_sequences[0][:20])\n",
    "print(\"Length of encoded review:\", len(train_sequences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ProposalDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        # for word_indicies in list_of_word_indicies, make it a tensory of long ints\n",
    "        self.sequences = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "        # list_of_word_indicies parallel array of ground truth labels\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "    \n",
    "train_dataset = ProposalDataset(train_sequences, train_labels)\n",
    "test_dataset =  ProposalDataset(test_sequences, test_labels)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Models\n",
    "\n",
    "We'll implement two models using PyTorch's nn.Module:\n",
    "\n",
    "1. LSTM-based RNN: An Embedding layer followed by an LSTM (recurrent layer) and a linear layer to output the sentiment class.\n",
    "2. Transformer Encoder: An Embedding layer (with added positional encoding) followed by Transformer encoder layers and a final linear layer for classification.\n",
    "\n",
    "Both models will output a prediction for each input review (binary classification: positive or negative). We will use a size-2 output (for classes 0 and 1) and later apply a softmax or use CrossEntropyLoss which expects raw logits of size 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim=100, num_heads=4, num_layers=2, ff_dim=256, max_len=250):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        # maps each word to a vector of size embed_dim\n",
    "        # each word gets an embedding vector, padding_idx=0 ensures padding token contributes nothing\n",
    "        # to learned representation\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        # each word gets a vector of size 100 to represent it's position in abstract, this will be learned\n",
    "        # these will be added to token embeddings\n",
    "        self.pos_embedding = nn.Embedding(max_len, embed_dim)  # learnable positional embeddings\n",
    "        # Define a Transformer Encoder layer (Multi-Headed Attention)\n",
    "        # contains both the self attention layer as well as the feed forward layer\n",
    "        # ff_dim is size of hidden layer in feedforward nn\n",
    "        # input shape is (batch, seq_len, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, \n",
    "                                                  dim_feedforward=ff_dim, batch_first=True)\n",
    "        # this stacks two encoder_layers ontop of each other\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # final representation to output two classes \n",
    "        self.fc = nn.Linear(embed_dim, 2)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_length)- seq_len should be 250 (number of abstract tokens)\n",
    "        batch_size, seq_len = x.size()\n",
    "        # positional encoding\n",
    "        # torch.arange(0, seq_len) createes a tensor of size seq_len-1 that has digits from 0 to seq_len\n",
    "        # torch.arange(0, seq_len).unsqueeze(0) adds a new dimension at index 0 -> (1,seq_len)\n",
    "        # torch.arange(0, seq_len).unsqueeze(0).expand(batch_size, seq_len) -> (batch_size, seq_len)\n",
    "        pos_indices = torch.arange(0, seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        # Get token embeddings and positional embeddings\n",
    "        token_embeds = self.embedding(x)            # (batch, seq_len, embed_dim)\n",
    "        pos_embeds = self.pos_embedding(pos_indices)  # (batch, seq_len, embed_dim)\n",
    "        # Add token and position embeddings\n",
    "        x_embedded = token_embeds + pos_embeds       # (batch, seq_len, embed_dim)\n",
    "        # Create padding mask (True where padding token is present)\n",
    "        pad_mask = (x == 0)  # shape: (batch, seq_len), True at padded indices\n",
    "        # Pass through Transformer encoder\n",
    "        enc_out = self.transformer(x_embedded, src_key_padding_mask=pad_mask)  # (batch, seq_len, embed_dim)\n",
    "        # Aggregate the encoder outputs; we use mean pooling\n",
    "        # for each sequence, get the average of all the token embeddings\n",
    "        seq_avg = enc_out.mean(dim=1)               # (batch, embed_dim)\n",
    "        # input into nn to get output\n",
    "        logits = self.fc(seq_avg)                   # (batch, 2)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Select device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Instantiate models\n",
    "vocab_size = len(word_to_idx)\n",
    "transformer_model = TransformerClassifier(vocab_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "trans_optimizer = optim.Adam(transformer_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Transformer Training loss: 0.6726\n",
      "Epoch 2/4, Transformer Training loss: 0.6112\n",
      "Epoch 3/4, Transformer Training loss: 0.4686\n",
      "Epoch 4/4, Transformer Training loss: 0.3212\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    transformer_model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        trans_optimizer.zero_grad()\n",
    "        outputs = transformer_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        trans_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Transformer Training loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Model Test Accuracy: 0.6257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frankwoods/anaconda3/envs/env3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:409: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538437738/work/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    }
   ],
   "source": [
    "transformer_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = transformer_model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "accuracy = correct / total\n",
    "print(f\"Transformer Model Test Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
